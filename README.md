# Informed-Learning guided Visual Question Answering Model of Crop Disease(ILCD)

## Abstract

In contemporary agriculture, experts frequently devise preventative and remedial strategies for various disease stages in diverse crops. Decision-making concerning disease occurrence stages exceeds the capability of single-image tasks like image classification and object detection. Efforts are now directed towards training visual question-answering (VQA) models utilizing images and text queries, aiming to address crop disease inquiries from a human perspective. However, existing studies primarily focus on identifying disease species rather than formulating questions encompassing multi-attributes crucial for decision-making during disease stages, model performance are prone to dataset biases. To address these challenges, we conducted the Informed-Learning guided Visual Question Answering Model of Crop Disease (ILCD). We enhance model performance by incorporating the Co-Attention and Multi-Modal Fusion Model (MUTAN) mechanisms, and mitigate single-peak dataset bias through a bias-balancing strategy. To enable ILCD to address questions regarding various visual attributes of crop diseases to determine disease occurrence stages, we conducted a new VQA dataset, crop disease multi-attribute VQA dataset with introduced prior knowledge (CDwPK-VQA), containing comprehensive information on visual attributes (e.g. shape, size, status, and color).  Utilizing an informed learning approach, we alleviate performance issues stemming from limited training data. ILCD achieves an accuracy of 68.90% and 49.75% on the VQA-v2 and VQA-CP v2 datasets, respectively. Notably, on CDwPK-VQA, ILCD achieves an accuracy of 86.06%, surpassing existing multimodal models.
